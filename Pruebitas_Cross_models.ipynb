{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fe8R3o-UQjlF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyaacoub/Cross-Domain-Attacks-NLP/blob/main/Pruebitas_Cross_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A63iXO3ls7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8471b84a-d131-447f-a3ae-a8a240635cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.5 MB 27.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 418 kB 56.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 64.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 22.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 60 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 365 kB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 401 kB 66.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 125 kB 56.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 769 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 63.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 67.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 95 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 69.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 61.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 788 kB 60.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 55.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 90.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 200 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 60.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 635 kB/s \n",
            "\u001b[?25h  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.4.1 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "gym 0.25.2 requires importlib-metadata>=4.8.0; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers torchinfo textattack -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchinfo import summary\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "8kXANxpLl9fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "gRzwXPRHX5Oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similar Domain - Same Task"
      ],
      "metadata": {
        "id": "fe8R3o-UQjlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"textattack/roberta-base-rotten-tomatoes\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "encoded_input = tokenizer(['I like you. I love you'], return_tensors='pt')\n",
        "summary(model, input_data=encoded_input.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65RLaRcWmXL0",
        "outputId": "f87e9f5e-bfdf-453f-ba2f-58f649b83ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-rotten-tomatoes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "RobertaForSequenceClassification                             [1, 2]                    --\n",
              "├─RobertaModel: 1-1                                          [1, 9, 768]               --\n",
              "│    └─RobertaEmbeddings: 2-1                                [1, 9, 768]               --\n",
              "│    │    └─Embedding: 3-1                                   [1, 9, 768]               38,603,520\n",
              "│    │    └─Embedding: 3-2                                   [1, 9, 768]               768\n",
              "│    │    └─Embedding: 3-3                                   [1, 9, 768]               394,752\n",
              "│    │    └─LayerNorm: 3-4                                   [1, 9, 768]               1,536\n",
              "│    │    └─Dropout: 3-5                                     [1, 9, 768]               --\n",
              "│    └─RobertaEncoder: 2-2                                   [1, 9, 768]               --\n",
              "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
              "├─RobertaClassificationHead: 1-2                             [1, 2]                    --\n",
              "│    └─Dropout: 2-3                                          [1, 768]                  --\n",
              "│    └─Linear: 2-4                                           [1, 768]                  590,592\n",
              "│    └─Dropout: 2-5                                          [1, 768]                  --\n",
              "│    └─Linear: 2-6                                           [1, 2]                    1,538\n",
              "==============================================================================================================\n",
              "Total params: 124,647,170\n",
              "Trainable params: 124,647,170\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 124.65\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 7.53\n",
              "Params size (MB): 498.59\n",
              "Estimated Total Size (MB): 506.12\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"textattack/roberta-base-imdb\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "encoded_input = tokenizer(['I like you. I love you'], return_tensors='pt')\n",
        "summary(model, input_data=encoded_input.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiL9EWNdp16Q",
        "outputId": "e0e9d26f-57aa-4ab3-9bb6-8d9f9eea346a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at textattack/roberta-base-imdb were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "RobertaForSequenceClassification                             [1, 2]                    --\n",
              "├─RobertaModel: 1-1                                          [1, 9, 768]               --\n",
              "│    └─RobertaEmbeddings: 2-1                                [1, 9, 768]               --\n",
              "│    │    └─Embedding: 3-1                                   [1, 9, 768]               38,603,520\n",
              "│    │    └─Embedding: 3-2                                   [1, 9, 768]               768\n",
              "│    │    └─Embedding: 3-3                                   [1, 9, 768]               394,752\n",
              "│    │    └─LayerNorm: 3-4                                   [1, 9, 768]               1,536\n",
              "│    │    └─Dropout: 3-5                                     [1, 9, 768]               --\n",
              "│    └─RobertaEncoder: 2-2                                   [1, 9, 768]               --\n",
              "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
              "├─RobertaClassificationHead: 1-2                             [1, 2]                    --\n",
              "│    └─Dropout: 2-3                                          [1, 768]                  --\n",
              "│    └─Linear: 2-4                                           [1, 768]                  590,592\n",
              "│    └─Dropout: 2-5                                          [1, 768]                  --\n",
              "│    └─Linear: 2-6                                           [1, 2]                    1,538\n",
              "==============================================================================================================\n",
              "Total params: 124,647,170\n",
              "Trainable params: 124,647,170\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 124.65\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 7.53\n",
              "Params size (MB): 498.59\n",
              "Estimated Total Size (MB): 506.12\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similar Domain, Different Task"
      ],
      "metadata": {
        "id": "ZjrBlS6yQ7cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"cardiffnlp/twitter-roberta-base-irony\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "encoded_input = tokenizer(['I like you. I love you'], return_tensors='pt')\n",
        "summary(model, input_data=encoded_input.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJJCukrrRH_D",
        "outputId": "a5566384-3bf9-4a52-cd02-5d90f33465b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "RobertaForSequenceClassification                             [1, 2]                    --\n",
              "├─RobertaModel: 1-1                                          [1, 9, 768]               --\n",
              "│    └─RobertaEmbeddings: 2-1                                [1, 9, 768]               --\n",
              "│    │    └─Embedding: 3-1                                   [1, 9, 768]               38,603,520\n",
              "│    │    └─Embedding: 3-2                                   [1, 9, 768]               768\n",
              "│    │    └─Embedding: 3-3                                   [1, 9, 768]               394,752\n",
              "│    │    └─LayerNorm: 3-4                                   [1, 9, 768]               1,536\n",
              "│    │    └─Dropout: 3-5                                     [1, 9, 768]               --\n",
              "│    └─RobertaEncoder: 2-2                                   [1, 9, 768]               --\n",
              "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
              "├─RobertaClassificationHead: 1-2                             [1, 2]                    --\n",
              "│    └─Dropout: 2-3                                          [1, 768]                  --\n",
              "│    └─Linear: 2-4                                           [1, 768]                  590,592\n",
              "│    └─Dropout: 2-5                                          [1, 768]                  --\n",
              "│    └─Linear: 2-6                                           [1, 2]                    1,538\n",
              "==============================================================================================================\n",
              "Total params: 124,647,170\n",
              "Trainable params: 124,647,170\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 124.65\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 7.53\n",
              "Params size (MB): 498.59\n",
              "Estimated Total Size (MB): 506.12\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "encoded_input = tokenizer(['I like you. I love you'], return_tensors='pt')\n",
        "summary(model, input_data=encoded_input.input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udtA8r27RqFO",
        "outputId": "f14738ed-c516-4a96-8f53-9b9298f91c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==============================================================================================================\n",
              "Layer (type:depth-idx)                                       Output Shape              Param #\n",
              "==============================================================================================================\n",
              "RobertaForSequenceClassification                             [1, 3]                    --\n",
              "├─RobertaModel: 1-1                                          [1, 9, 768]               --\n",
              "│    └─RobertaEmbeddings: 2-1                                [1, 9, 768]               --\n",
              "│    │    └─Embedding: 3-1                                   [1, 9, 768]               38,603,520\n",
              "│    │    └─Embedding: 3-2                                   [1, 9, 768]               768\n",
              "│    │    └─Embedding: 3-3                                   [1, 9, 768]               394,752\n",
              "│    │    └─LayerNorm: 3-4                                   [1, 9, 768]               1,536\n",
              "│    │    └─Dropout: 3-5                                     [1, 9, 768]               --\n",
              "│    └─RobertaEncoder: 2-2                                   [1, 9, 768]               --\n",
              "│    │    └─ModuleList: 3-6                                  --                        85,054,464\n",
              "├─RobertaClassificationHead: 1-2                             [1, 3]                    --\n",
              "│    └─Dropout: 2-3                                          [1, 768]                  --\n",
              "│    └─Linear: 2-4                                           [1, 768]                  590,592\n",
              "│    └─Dropout: 2-5                                          [1, 768]                  --\n",
              "│    └─Linear: 2-6                                           [1, 3]                    2,307\n",
              "==============================================================================================================\n",
              "Total params: 124,647,939\n",
              "Trainable params: 124,647,939\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 124.65\n",
              "==============================================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 7.53\n",
              "Params size (MB): 498.59\n",
              "Estimated Total Size (MB): 506.12\n",
              "=============================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attacks"
      ],
      "metadata": {
        "id": "tMlHl8-aX_wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textattack\n",
        "import transformers\n",
        "\n",
        "from textattack import Attack\n",
        "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
        "from textattack.constraints.semantics import WordEmbeddingDistance\n",
        "from textattack.transformations import WordSwapEmbedding\n",
        "from textattack.search_methods import GreedyWordSwapWIR\n",
        "\n",
        "# Load model, tokenizer, and model_wrapper\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
        "model = model.cuda()\n",
        "# print(next(model.parameters()).is_cuda) # True\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
        "model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "# Construct our four components for `Attack`\n",
        "goal_function = textattack.goal_functions.UntargetedClassification(model_wrapper)\n",
        "constraints = [\n",
        "    RepeatModification(),\n",
        "    StopwordModification(),\n",
        "    WordEmbeddingDistance(min_cos_sim=0.9)\n",
        "]\n",
        "transformation = WordSwapEmbedding(max_candidates=50)\n",
        "search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
        "\n",
        "# Construct the actual attack\n",
        "attack = Attack(goal_function, constraints, transformation, search_method)\n",
        "\n",
        "input_text = \"I really enjoyed the new movie that came out last month, what about you?.\"\n",
        "label = 1    # Positive\n",
        "attack_result = attack.attack(input_text, label)\n",
        "print(attack_result.__str__(color_method='ansi'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6893PBH5YAw8",
        "outputId": "6b501e7d-b675-4620-c03b-50fca04cbd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m1 (81%)\u001b[0m --> \u001b[91m[FAILED]\u001b[0m\n",
            "\n",
            "I really enjoyed the new movie that came out last month, what about you?.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Load model, tokenizer, and model_wrapper\n",
        "# model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-imdb\")\n",
        "# model_wrapper = textattack.models.wrappers.HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "# Construct our four components for `Attack`\n",
        "goal_function = textattack.goal_functions.UntargetedClassification(model_wrapper)\n",
        "constraints = [\n",
        "    RepeatModification(),\n",
        "    StopwordModification(),\n",
        "    WordEmbeddingDistance(min_cos_sim=0.9)\n",
        "]\n",
        "transformation = WordSwapEmbedding(max_candidates=50)\n",
        "search_method = GreedyWordSwapWIR(wir_method=\"delete\")\n",
        "\n",
        "# Construct the actual attack\n",
        "attack = Attack(goal_function, constraints, transformation, search_method)\n",
        "\n",
        "input_text = \"I really enjoyed the new movie that came out last month, what about you?.\"\n",
        "label = 1    # Positive\n",
        "attack_result = attack.attack(input_text, label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnbvSZkMbpPO",
        "outputId": "a68538e8-973a-45ba-83b2-ca64feff3391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "textattack: Unknown if model of class <class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.37 s, sys: 208 ms, total: 1.58 s\n",
            "Wall time: 3.48 s\n"
          ]
        }
      ]
    }
  ]
}